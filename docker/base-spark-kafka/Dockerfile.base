# Use the official Spark Py image
FROM apache/spark-py:v3.4.0

# Switch to root to modify directories
USER root

# Set working directory for downloading JARs
WORKDIR /opt/spark/jars

# Download Kafka integration JARs directly to Spark's jars directory
RUN wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.0/spark-sql-kafka-0-10_2.12-3.4.0.jar
RUN wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0.jar
RUN wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar
RUN wget https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.4.0/spark-token-provider-kafka-0-10_2.12-3.4.0.jar

# Ensure permissions are set correctly
RUN chmod -R 777 /opt/spark/jars

# Copy Minikube's CA certificate into the container
COPY minikube-ca.crt /usr/local/share/ca-certificates/

# Update the system CA trust store to include the Minikube CA
RUN update-ca-certificates

# Add Minikube's CA certificate to the JVM trust store
RUN keytool -importcert -trustcacerts \
    -file /usr/local/share/ca-certificates/minikube-ca.crt \
    -alias minikube-ca \
    -keystore /opt/java/openjdk/lib/security/cacerts \
    -storepass changeit -noprompt

# Ensure the /opt/spark/conf directory exists before copying files
RUN mkdir -p /opt/spark/conf

# Copy external configuration files to the container
COPY spark-defaults.conf /opt/spark/conf/spark-defaults.conf
COPY log4j.properties /opt/spark/conf/log4j.properties

# Ensure permissions for configuration files
RUN chmod -R 777 /opt/spark/conf

# Create necessary directories for Spark file uploads & logs
RUN mkdir -p /tmp/spark-upload /mnt/spark/logs && chmod -R 777 /mnt/spark/logs

# Set SPARK_CLASSPATH to include all JARs in the directory
ENV SPARK_CLASSPATH="/opt/spark/jars/*"

# Switch back to the default non-root user
USER 185

# Base image is ready to be extended
CMD ["python", "--version"]
